{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fa10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Module\n",
    "from torch import softmax\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bb49e",
   "metadata": {},
   "source": [
    "## Basic Self Attention\n",
    "\n",
    "\n",
    "- input X, shape is (bs, seqlen, hidden_size)\n",
    "- weight Wq, Wk, Wv, shape is (hidden_size， hidden_size) \n",
    "- Q,K,V shape is (bs, seqlen, hidden_size)\n",
    "- attention_score shape is (bs, seqlen, seqlen), second dim is each token, third dim is the score of all tokens against this token \n",
    "- attention_output shape is (bs, seqlen, hidden_size)\n",
    "\n",
    "note: \n",
    "`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`\n",
    "Applies an affine linear transformation to the incoming data: \n",
    "```y=xA^T+b```\n",
    "\n",
    "to calculate Q K^T, need:\n",
    "- transpose K on last 2 dims (seqlen and hidden_size)\n",
    "- use matmul (@)\n",
    "\n",
    "note:\n",
    "`torch.matmul(input, other, *, out=None)`\n",
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "- If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "- If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "- If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "- If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a **batched matrix multiply** is returned. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wk = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wv = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wo = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.scale = hidden_size ** (1/2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        # some model apply position embedding here\n",
    "        # cos, sin = self.rotary_emb(V, seq_len=seq_len)\n",
    "        # Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)\n",
    "        \n",
    "        attention_score = softmax(Q @ K.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        return self.Wo(attention_score @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0d43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttention(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8b6d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1180,  0.2132,  0.1698,  ..., -0.2668, -0.2701, -0.0347],\n",
       "         [-0.1179,  0.2134,  0.1706,  ..., -0.2672, -0.2701, -0.0357],\n",
       "         [-0.1184,  0.2139,  0.1715,  ..., -0.2665, -0.2702, -0.0363],\n",
       "         ...,\n",
       "         [-0.1179,  0.2135,  0.1718,  ..., -0.2665, -0.2703, -0.0350],\n",
       "         [-0.1187,  0.2144,  0.1706,  ..., -0.2672, -0.2704, -0.0350],\n",
       "         [-0.1178,  0.2132,  0.1705,  ..., -0.2668, -0.2698, -0.0352]],\n",
       "\n",
       "        [[-0.1145,  0.1760,  0.1555,  ..., -0.2653, -0.2918, -0.0416],\n",
       "         [-0.1145,  0.1761,  0.1544,  ..., -0.2650, -0.2915, -0.0424],\n",
       "         [-0.1147,  0.1752,  0.1550,  ..., -0.2648, -0.2924, -0.0427],\n",
       "         ...,\n",
       "         [-0.1136,  0.1760,  0.1560,  ..., -0.2648, -0.2920, -0.0419],\n",
       "         [-0.1148,  0.1762,  0.1554,  ..., -0.2646, -0.2922, -0.0423],\n",
       "         [-0.1141,  0.1754,  0.1560,  ..., -0.2648, -0.2918, -0.0421]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b75df",
   "metadata": {},
   "source": [
    "## MHA\n",
    "\n",
    "\n",
    "- the Wq Qk Wv are still the shape (hidden_size, hidden_size) on init, but will reshape to (hidden_size, num_heads, head_dim) when calling forward \n",
    "- Q K V shape will be (batch_size, seqlen, num_heads, head_dim)\n",
    "- Q K^T needs output of (batch_size, num_heads, seqlen, seqlen), so transpose seqlen and num_heads\n",
    "- scale use head_dim, not hidden_size\n",
    "- concat all heads after attention_score @ V (batch_size, num_heads, seqlen, head_dim) -> (batch_size, seqlen, num_heads * head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26f285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(SelfAttention):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__(hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = self.head_dim ** (1/2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seqlen, _ = x.shape\n",
    "        Q = self.Wq(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        K = self.Wk(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        V = self.Wv(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        \n",
    "        attention_score = softmax(Q.transpose(1,2) @ K.transpose(1,2).transpose(-1, -2) / self.scale, dim=-1)\n",
    "        attention_output = (attention_score @ V.transpose(1,2)).transpose(1,2).contiguous().view(bs, seqlen, -1)\n",
    "        return self.Wo(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07eedb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = MultiHeadSelfAttention(512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7442a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1221,  0.0139, -0.1489,  ..., -0.1508,  0.2944, -0.0723],\n",
       "         [ 0.1221,  0.0140, -0.1486,  ..., -0.1510,  0.2942, -0.0720],\n",
       "         [ 0.1227,  0.0129, -0.1486,  ..., -0.1503,  0.2948, -0.0726],\n",
       "         ...,\n",
       "         [ 0.1217,  0.0135, -0.1492,  ..., -0.1502,  0.2945, -0.0721],\n",
       "         [ 0.1222,  0.0131, -0.1487,  ..., -0.1500,  0.2934, -0.0715],\n",
       "         [ 0.1216,  0.0141, -0.1493,  ..., -0.1506,  0.2947, -0.0721]],\n",
       "\n",
       "        [[ 0.1366,  0.0156, -0.1249,  ..., -0.1414,  0.2669, -0.0616],\n",
       "         [ 0.1363,  0.0158, -0.1251,  ..., -0.1407,  0.2666, -0.0614],\n",
       "         [ 0.1361,  0.0157, -0.1247,  ..., -0.1406,  0.2664, -0.0623],\n",
       "         ...,\n",
       "         [ 0.1355,  0.0171, -0.1241,  ..., -0.1416,  0.2660, -0.0616],\n",
       "         [ 0.1362,  0.0158, -0.1249,  ..., -0.1411,  0.2662, -0.0621],\n",
       "         [ 0.1356,  0.0159, -0.1242,  ..., -0.1404,  0.2664, -0.0619]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354b276",
   "metadata": {},
   "source": [
    "## GQA\n",
    "\n",
    "- the Wq is the original size, Wk and Wv should be smaller\n",
    "- Wq is reshaped as (hidden_size, num_heads, head_dim), Wk and Wv is reshaped as (hidden_size, num_kv_heads, head_dim)\n",
    "- Q is (bs, seqlen, num_heads, head_dim), KV is  (bs, seqlen, num_kv_heads, head_dim), so need to interleave KV for group_size times, after repeating, KV is (bs, seqlen, num_heads, head_dim)\n",
    "- attention_score is still (bs, num_heads, seqlen, seqlen), after matmul V^T, is (bs, num_heads, seqlen, head_dim)\n",
    "\n",
    "note:\n",
    "`torch.repeat_interleave(input, repeats, dim=None, *, output_size=None)`\n",
    "\n",
    "- input (Tensor) – the input tensor.\n",
    "- repeats (Tensor or int) – The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.\n",
    "- dim (int, optional) – The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.\n",
    "\n",
    "```\n",
    ">>> torch.repeat_interleave(y, 3, dim=1)\n",
    "tensor([[1, 1, 1, 2, 2, 2],\n",
    "        [3, 3, 3, 4, 4, 4]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f3080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = self.head_dim ** (1/2)\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.group_size = num_heads // num_kv_heads\n",
    "        \n",
    "        self.Wq = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wk = Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = Linear(hidden_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seqlen, _ = x.shape\n",
    "        Q = self.Wq(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        K = self.Wk(x).view(bs, seqlen, self.num_kv_heads, self.head_dim)\n",
    "        V = self.Wv(x).view(bs, seqlen, self.num_kv_heads, self.head_dim)\n",
    "        \n",
    "        K = K.repeat_interleave(self.group_size, dim=2)\n",
    "        V = V.repeat_interleave(self.group_size, dim=2)\n",
    "        \n",
    "        attention_score = (Q.transpose(1,2) @ K.transpose(1,2).transpose(-1,-2) / self.scale)\n",
    "        attention_output = (attention_score @ V.transpose(1,2)).transpose(1,2).contiguous().view(bs, seqlen, -1)\n",
    "        return self.Wo(attention_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246c9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = GroupedQueryAttention(512, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cba9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0130e-01, -2.2926e-03, -5.7200e-01,  ...,  2.5513e-01,\n",
       "           7.5714e-01, -3.3590e-01],\n",
       "         [ 7.3070e-01,  4.1067e-01,  3.1527e-01,  ...,  6.7676e-01,\n",
       "           5.1814e-01, -5.2487e-02],\n",
       "         [ 1.6133e-01,  2.3641e-01, -2.5082e-03,  ...,  4.0818e-01,\n",
       "           5.3284e-01, -5.2782e-02],\n",
       "         ...,\n",
       "         [ 3.2272e-01,  8.3971e-01,  2.9630e-02,  ...,  6.1965e-01,\n",
       "          -1.9962e-02,  1.3014e-01],\n",
       "         [ 2.2647e-01,  6.8219e-01, -4.7240e-01,  ...,  5.8010e-01,\n",
       "           3.3260e-01, -1.9878e-03],\n",
       "         [-4.8943e-02,  2.1508e-01, -3.7732e-01,  ...,  2.0345e-01,\n",
       "           3.0802e-01, -1.1944e-01]],\n",
       "\n",
       "        [[ 5.9472e-01,  4.0098e-01, -1.4606e-01,  ..., -1.6119e-01,\n",
       "           6.3656e-01, -4.6469e-04],\n",
       "         [-3.2492e-03,  5.7523e-01, -8.7522e-02,  ...,  3.2010e-01,\n",
       "           5.9767e-01,  1.5955e-01],\n",
       "         [ 3.6261e-01,  5.3366e-01, -8.5597e-02,  ...,  5.9380e-01,\n",
       "           7.7183e-01,  1.5350e-01],\n",
       "         ...,\n",
       "         [ 5.9433e-01,  3.4048e-01,  5.4644e-01,  ...,  4.6501e-01,\n",
       "           5.5233e-01, -1.1862e-01],\n",
       "         [ 7.5617e-01,  3.4131e-01, -2.2191e-01,  ...,  7.7463e-01,\n",
       "           4.7561e-01, -1.4247e-01],\n",
       "         [ 4.1838e-01,  3.7983e-01, -4.8311e-01,  ...,  4.7569e-01,\n",
       "           6.5856e-01, -2.1931e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaed9e1",
   "metadata": {},
   "source": [
    "## KV Cache\n",
    "\n",
    "- the forward just need to handle 1 token at a time, means seqlen==1, so the input X shape is (bs, 1, hidden_size)\n",
    "- past_kv = (past_key, past_value), both in shape of (bs, seqlen, hidden_size)\n",
    "- for new kv, concat past and this token's on dim=1, so now the Q is (bs, 1, hidden_size), new KV is (bs, seqlen+1, hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650534c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWithKVCache(SelfAttention):\n",
    "    \n",
    "    def forward(self, x, cached_kv=None):\n",
    "        bs, _, hidden_size = x.shape\n",
    "        if cached_kv:\n",
    "            cached_key, cached_value = cached_kv\n",
    "        else:\n",
    "            cached_key = torch.zeros(bs, 0, hidden_size)\n",
    "            cached_value = torch.zeros(bs, 0, hidden_size)\n",
    "        \n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        cached_key = torch.cat([cached_key, K], dim=1)\n",
    "        cached_value = torch.cat([cached_value, V], dim=1)\n",
    "        \n",
    "        attention_score = softmax(Q @ cached_key.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        output = self.Wo(attention_score @ cached_value)\n",
    "        \n",
    "        return output, (cached_key, cached_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b08801",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttentionWithKVCache(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8f54e",
   "metadata": {},
   "source": [
    "### Time with KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68dbb1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1573340892791748\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "output, kv_cache = layer(torch.rand(2, 4096, 512))\n",
    "prefilling_time_cost = time.time() - start_time\n",
    "print(prefilling_time_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aad836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011071920394897461\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4097, 512]) torch.Size([2, 4097, 512])\n",
      "0.005232810974121094\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4098, 512]) torch.Size([2, 4098, 512])\n",
      "0.0035789012908935547\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4099, 512]) torch.Size([2, 4099, 512])\n",
      "0.003389120101928711\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4100, 512]) torch.Size([2, 4100, 512])\n",
      "0.006175041198730469\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4101, 512]) torch.Size([2, 4101, 512])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    start_time = time.time()\n",
    "    output, kv_cache = layer(torch.rand(2, 1, 512), kv_cache)\n",
    "    decoding_time_cost = time.time() - start_time\n",
    "    print(decoding_time_cost)\n",
    "    print(output.shape, kv_cache[0].shape, kv_cache[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee4626",
   "metadata": {},
   "source": [
    "### Time without KV Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34bfdd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13533520698547363\n",
      "torch.Size([2, 4096, 512])\n",
      "0.1895592212677002\n",
      "torch.Size([2, 4097, 512])\n",
      "0.18368291854858398\n",
      "torch.Size([2, 4098, 512])\n",
      "0.14872288703918457\n",
      "torch.Size([2, 4099, 512])\n",
      "0.14966773986816406\n",
      "torch.Size([2, 4100, 512])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    start_time = time.time()\n",
    "    output, _ = layer(torch.rand(2, 4096 + _, 512), None)\n",
    "    decoding_time_cost = time.time() - start_time\n",
    "    print(decoding_time_cost)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be7120",
   "metadata": {},
   "source": [
    "## Heterogeneous Attention\n",
    "\n",
    "\n",
    "- the code demo is based on basic self attention, instead of the original implementation on GQA\n",
    "- the prefilled K and V both in shape (bs, seqlen, hidden_size)\n",
    "\n",
    "note:\n",
    "\n",
    "`torch.index_select(input, dim, index, *, out=None) → Tensor`\n",
    "\n",
    "- Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.\n",
    "- The returned tensor has the same number of dimensions as the original tensor (input). The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f22c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousKVCache:\n",
    "    def __init__(self, layer_idx):\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        self.sink_token_count = 128\n",
    "        self.recent_token_count = 128\n",
    "        \n",
    "        self.sink_kv = None\n",
    "        self.context_kv = None\n",
    "        self.recent_kv = None\n",
    "        \n",
    "        self.is_prefilled = False\n",
    "        \n",
    "    \n",
    "    def offload(self, prefilled_kv):\n",
    "        assert self.is_prefilled is False\n",
    "        prefilled_keys, prefilled_values = prefilled_kv # assume the seqlen is long enough, e.g. 100K\n",
    "        \n",
    "        self.sink_kv = (prefilled_keys[:, :self.sink_token_count, :], prefilled_values[:, :self.sink_token_count, :])\n",
    "        self.context_kv = (prefilled_keys[:, self.sink_token_count: -self.sink_token_count, :], prefilled_values[:, self.sink_token_count: -self.sink_token_count, :])\n",
    "        self.recent_kv = (prefilled_keys[:, -self.sink_token_count:, :], prefilled_values[:, -self.sink_token_count:, :])\n",
    "            \n",
    "        \n",
    "        if self.layer_idx >= 2 and self.layer_idx < 31: # assume this is a 32-layer model\n",
    "            context_keys, context_values = self.context_kv\n",
    "            # context_keys = context_keys.cpu()\n",
    "            # context_keys = build_vector_store(context_keys)\n",
    "        \n",
    "        self.is_prefilled = True\n",
    "            \n",
    "    \n",
    "    def update(self, new_recent_kv):\n",
    "        assert self.is_prefilled is True\n",
    "        new_recent_keys, new_recent_values = new_recent_kv\n",
    "        self.recent_kv = (\n",
    "            torch.cat([self.recent_kv[0], new_recent_keys], dim=1), \n",
    "            torch.cat([self.recent_kv[1], new_recent_values], dim=1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def search(self, query, top_k):\n",
    "        assert self.is_prefilled is True\n",
    "        \n",
    "        # selected_indices = search_vector_store(query, top_k)\n",
    "        \n",
    "        selected_keys = torch.index_select(self.context_kv[0], 1, selected_indices).cuda()\n",
    "        selected_values = torch.index_select(self.context_kv[1], 1, selected_indices)\n",
    "        \n",
    "        return (\n",
    "            torch.cat([self.sink_kv[0], selected_keys, self.recent_kv[0]], dim=1), \n",
    "            torch.cat([self.sink_kv[1], selected_values, self.recent_kv[1]], dim=1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "class HeterogeneousAttention(SelfAttentionWithKVCache): # or: SelfAttentionWithHeterogeneousKVCache\n",
    "    def __init__(self, hidden_size, layer_idx):\n",
    "        super().__init__(hidden_size)\n",
    "        self.heterogeneous_kv_cache = HeterogeneousKVCache(layer_idx)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        if not self.heterogeneous_kv_cache.is_prefilled:\n",
    "            K = self.Wk(x)\n",
    "            V = self.Wk(x)\n",
    "        else:\n",
    "            (K, V) = self.heterogeneous_kv_cache.search(Q, top_k=16)\n",
    "            \n",
    "        \n",
    "        attention_score = softmax(Q @ K.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        output = self.Wo(attention_score @ V)\n",
    "        \n",
    "        if not self.heterogeneous_kv_cache.is_prefilled:\n",
    "            self.heterogeneous_kv_cache.offload((K, V))\n",
    "        else:\n",
    "            self.heterogeneous_kv_cache.update((K[:, -1, :], V[:, -1, :]))\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6fa10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bb49e",
   "metadata": {},
   "source": [
    "## Basic Self Attention\n",
    "\n",
    "\n",
    "- input X, shape is (bs, seqlen, hidden_size)\n",
    "- weight Wq, Wk, Wv, shape is (hidden_size， hidden_size) \n",
    "- Q,K,V shape is (bs, seqlen, hidden_size)\n",
    "- attention_score shape is (bs, seqlen, seqlen), second dim is each token, third dim is the score of all tokens against this token \n",
    "- attention_output shape is (bs, seqlen, hidden_size)\n",
    "\n",
    "note: \n",
    "`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`\n",
    "Applies an affine linear transformation to the incoming data: \n",
    "```y=xA^T+b```\n",
    "\n",
    "to calculate Q K^T, need:\n",
    "- transpose K on last 2 dims (seqlen and hidden_size)\n",
    "- use matmul (@)\n",
    "\n",
    "note:\n",
    "`torch.matmul(input, other, *, out=None)`\n",
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "- If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "- If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "- If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "- If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a **batched matrix multiply** is returned. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "568d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wk = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wv = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wo = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.scale = hidden_size ** (1/2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        attention_score = softmax(Q @ K.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        return self.Wo(attention_score @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd0d43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttention(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd8b6d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3673e-01,  1.7024e-01, -1.0274e-02,  ...,  4.1232e-02,\n",
       "          -9.1783e-02, -4.5934e-06],\n",
       "         [ 1.3637e-01,  1.7086e-01, -1.0617e-02,  ...,  4.1984e-02,\n",
       "          -9.0352e-02,  5.8779e-04],\n",
       "         [ 1.3568e-01,  1.7027e-01, -1.0546e-02,  ...,  4.1324e-02,\n",
       "          -9.0808e-02,  2.9186e-05],\n",
       "         ...,\n",
       "         [ 1.3546e-01,  1.6903e-01, -9.2836e-03,  ...,  4.1253e-02,\n",
       "          -9.0734e-02, -1.0160e-03],\n",
       "         [ 1.3652e-01,  1.6946e-01, -1.0524e-02,  ...,  4.1064e-02,\n",
       "          -9.0825e-02, -6.5265e-04],\n",
       "         [ 1.3561e-01,  1.6883e-01, -9.5603e-03,  ...,  4.0815e-02,\n",
       "          -9.0803e-02, -9.8485e-04]],\n",
       "\n",
       "        [[ 1.1687e-01,  2.2417e-01,  3.0953e-02,  ...,  2.4701e-02,\n",
       "          -7.5829e-02, -1.6887e-02],\n",
       "         [ 1.1527e-01,  2.2471e-01,  3.1047e-02,  ...,  2.4696e-02,\n",
       "          -7.6358e-02, -1.7282e-02],\n",
       "         [ 1.1688e-01,  2.2457e-01,  3.1124e-02,  ...,  2.5412e-02,\n",
       "          -7.5537e-02, -1.6291e-02],\n",
       "         ...,\n",
       "         [ 1.1584e-01,  2.2506e-01,  3.0873e-02,  ...,  2.4758e-02,\n",
       "          -7.5628e-02, -1.6925e-02],\n",
       "         [ 1.1603e-01,  2.2465e-01,  3.1312e-02,  ...,  2.5296e-02,\n",
       "          -7.6356e-02, -1.6875e-02],\n",
       "         [ 1.1597e-01,  2.2483e-01,  3.0921e-02,  ...,  2.4746e-02,\n",
       "          -7.6302e-02, -1.6685e-02]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b75df",
   "metadata": {},
   "source": [
    "## MHA\n",
    "\n",
    "\n",
    "- the Wq Qk Wv are still the shape (hidden_size, hidden_size) on init, but will reshape to (hidden_size, num_heads, head_dim) when calling forward \n",
    "- Q K V shape will be (batch_size, seqlen, num_heads, head_dim)\n",
    "- Q K^T needs output of (batch_size, num_heads, seqlen, seqlen), so transpose seqlen and num_heads\n",
    "- scale use head_dim, not hidden_size\n",
    "- concat all heads after attention_score @ V (batch_size, num_heads, seqlen, head_dim) -> (batch_size, seqlen, num_heads * head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d26f285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(SelfAttention):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__(hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = self.head_dim ** (1/2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seqlen, _ = x.shape\n",
    "        Q = self.Wq(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        K = self.Wk(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        V = self.Wv(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        \n",
    "        attention_score = softmax(Q.transpose(1,2) @ K.transpose(1,2).transpose(-1, -2) / self.scale, dim=-1)\n",
    "        attention_output = (attention_score @ V.transpose(1,2)).transpose(1,2).contiguous().view(bs, seqlen, -1)\n",
    "        return self.Wo(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07eedb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = MultiHeadSelfAttention(512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad7442a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0442, -0.0179, -0.0077,  ..., -0.1054, -0.1257, -0.0280],\n",
       "         [-0.0439, -0.0180, -0.0084,  ..., -0.1055, -0.1254, -0.0277],\n",
       "         [-0.0448, -0.0184, -0.0087,  ..., -0.1060, -0.1256, -0.0284],\n",
       "         ...,\n",
       "         [-0.0456, -0.0191, -0.0094,  ..., -0.1060, -0.1261, -0.0274],\n",
       "         [-0.0442, -0.0183, -0.0090,  ..., -0.1062, -0.1251, -0.0286],\n",
       "         [-0.0442, -0.0196, -0.0081,  ..., -0.1061, -0.1250, -0.0280]],\n",
       "\n",
       "        [[-0.0682, -0.0174, -0.0067,  ..., -0.1155, -0.1311, -0.0311],\n",
       "         [-0.0677, -0.0164, -0.0067,  ..., -0.1149, -0.1304, -0.0315],\n",
       "         [-0.0682, -0.0174, -0.0074,  ..., -0.1163, -0.1311, -0.0319],\n",
       "         ...,\n",
       "         [-0.0688, -0.0165, -0.0064,  ..., -0.1146, -0.1304, -0.0309],\n",
       "         [-0.0678, -0.0162, -0.0065,  ..., -0.1152, -0.1309, -0.0316],\n",
       "         [-0.0681, -0.0164, -0.0068,  ..., -0.1150, -0.1300, -0.0312]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354b276",
   "metadata": {},
   "source": [
    "## GQA\n",
    "\n",
    "- the Wq is the original size, Wk and Wv should be smaller\n",
    "- Wq is reshaped as (hidden_size, num_heads, head_dim), Wk and Wv is reshaped as (hidden_size, num_kv_heads, head_dim)\n",
    "- Q is (bs, seqlen, num_heads, head_dim), KV is  (bs, seqlen, num_kv_heads, head_dim), so need to interleave KV for group_size times, after repeating, KV is (bs, seqlen, num_heads, head_dim)\n",
    "- attention_score is still (bs, num_heads, seqlen, seqlen), after matmul V^T, is (bs, num_heads, seqlen, head_dim)\n",
    "\n",
    "note:\n",
    "`torch.repeat_interleave(input, repeats, dim=None, *, output_size=None)`\n",
    "\n",
    "- input (Tensor) – the input tensor.\n",
    "- repeats (Tensor or int) – The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.\n",
    "- dim (int, optional) – The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.\n",
    "\n",
    "```\n",
    ">>> torch.repeat_interleave(y, 3, dim=1)\n",
    "tensor([[1, 1, 1, 2, 2, 2],\n",
    "        [3, 3, 3, 4, 4, 4]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5f3080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = self.head_dim ** (1/2)\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.group_size = num_heads // num_kv_heads\n",
    "        \n",
    "        self.Wq = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wk = Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = Linear(hidden_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seqlen, _ = x.shape\n",
    "        Q = self.Wq(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        K = self.Wk(x).view(bs, seqlen, self.num_kv_heads, self.head_dim)\n",
    "        V = self.Wv(x).view(bs, seqlen, self.num_kv_heads, self.head_dim)\n",
    "        \n",
    "        K = K.repeat_interleave(self.group_size, dim=2)\n",
    "        V = V.repeat_interleave(self.group_size, dim=2)\n",
    "        \n",
    "        attention_score = (Q.transpose(1,2) @ K.transpose(1,2).transpose(-1,-2) / self.scale)\n",
    "        attention_output = (attention_score @ V.transpose(1,2)).transpose(1,2).contiguous().view(bs, seqlen, -1)\n",
    "        return self.Wo(attention_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "246c9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = GroupedQueryAttention(512, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d4cba9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2463, -0.3342, -0.3047,  ...,  0.2547, -0.0646,  1.3266],\n",
       "         [-0.0288, -0.7129,  0.0258,  ...,  0.1388, -0.1351,  1.5268],\n",
       "         [-0.0497, -0.6961,  0.1443,  ...,  0.3307, -0.3921,  1.3867],\n",
       "         ...,\n",
       "         [ 0.0371, -1.0292, -0.0592,  ...,  0.0646,  0.2182,  1.7566],\n",
       "         [ 0.0039, -0.7354, -0.1269,  ..., -0.2023, -0.1142,  1.2810],\n",
       "         [-0.0516, -0.8042, -0.5209,  ...,  0.1557, -0.6026,  1.0158]],\n",
       "\n",
       "        [[ 0.0509, -1.1555,  0.1331,  ...,  0.5010, -0.3044,  1.6570],\n",
       "         [-0.4592, -1.1015, -0.4584,  ...,  0.9297, -0.1316,  1.2649],\n",
       "         [ 0.4306, -0.8634,  0.0858,  ...,  0.3441, -0.1953,  1.2412],\n",
       "         ...,\n",
       "         [-0.2646, -0.9186,  0.0988,  ...,  0.4202, -0.0237,  1.4110],\n",
       "         [-0.3137, -0.5333, -0.4952,  ...,  0.4895, -0.0046,  1.2702],\n",
       "         [ 0.0539, -0.9006,  0.4146,  ...,  0.3690, -0.0928,  1.2824]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaed9e1",
   "metadata": {},
   "source": [
    "## KV Cache\n",
    "\n",
    "- the forward just need to handle 1 token at a time, means seqlen==1, so the input X shape is (bs, 1, hidden_size)\n",
    "- past_kv = (past_key, past_value), both in shape of (bs, seqlen, hidden_size)\n",
    "- for new kv, concat past and this token's on dim=1, so now the Q is (bs, 1, hidden_size), new KV is (bs, seqlen+1, hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "650534c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWithKVCache(SelfAttention):\n",
    "    \n",
    "    def forward(self, x, cached_kv=None):\n",
    "        bs, _, hidden_size = x.shape\n",
    "        if cached_kv:\n",
    "            cached_key, cached_value = cached_kv\n",
    "        else:\n",
    "            cached_key = torch.zeros(bs, 0, hidden_size)\n",
    "            cached_value = torch.zeros(bs, 0, hidden_size)\n",
    "        \n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        cached_key = torch.cat([cached_key, K], dim=1)\n",
    "        cached_value = torch.cat([cached_value, V], dim=1)\n",
    "        \n",
    "        attention_score = softmax(Q @ cached_key.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        output = self.Wo(attention_score @ cached_value)\n",
    "        \n",
    "        return output, (cached_key, cached_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c4b08801",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttentionWithKVCache(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "68dbb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, kv_cache = layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8aad836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "595d6673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 32, 512]), torch.Size([2, 32, 512]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache[0].shape, kv_cache[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "608a9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, kv_cache = layer(torch.rand(2, 1, 512), kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3123667d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 512])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8308c94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 33, 512]), torch.Size([2, 33, 512]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache[0].shape, kv_cache[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033b3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22c569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6fa10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Module\n",
    "from torch import softmax\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bb49e",
   "metadata": {},
   "source": [
    "## Basic Self Attention\n",
    "\n",
    "\n",
    "- input X, shape is (bs, seqlen, hidden_size)\n",
    "- weight Wq, Wk, Wv, shape is (hidden_size， hidden_size) \n",
    "- Q,K,V shape is (bs, seqlen, hidden_size)\n",
    "- attention_score shape is (bs, seqlen, seqlen), second dim is each token, third dim is the score of all tokens against this token \n",
    "- attention_output shape is (bs, seqlen, hidden_size)\n",
    "\n",
    "note: \n",
    "`torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)`\n",
    "Applies an affine linear transformation to the incoming data: \n",
    "```y=xA^T+b```\n",
    "\n",
    "to calculate Q K^T, need:\n",
    "- transpose K on last 2 dims (seqlen and hidden_size)\n",
    "- use matmul (@)\n",
    "\n",
    "note:\n",
    "`torch.matmul(input, other, *, out=None)`\n",
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "- If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "- If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "- If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "- If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a **batched matrix multiply** is returned. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wk = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wv = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wo = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.scale = hidden_size ** (1/2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        attention_score = softmax(Q @ K.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        return self.Wo(attention_score @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0d43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttention(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8b6d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0395, -0.0129, -0.1174,  ...,  0.3310, -0.0522,  0.2033],\n",
       "         [-0.0397, -0.0138, -0.1185,  ...,  0.3312, -0.0536,  0.2036],\n",
       "         [-0.0392, -0.0130, -0.1186,  ...,  0.3306, -0.0531,  0.2035],\n",
       "         ...,\n",
       "         [-0.0401, -0.0143, -0.1181,  ...,  0.3309, -0.0520,  0.2033],\n",
       "         [-0.0388, -0.0135, -0.1184,  ...,  0.3307, -0.0543,  0.2032],\n",
       "         [-0.0397, -0.0141, -0.1185,  ...,  0.3302, -0.0529,  0.2038]],\n",
       "\n",
       "        [[-0.0096, -0.0171, -0.1130,  ...,  0.3027, -0.0272,  0.2473],\n",
       "         [-0.0110, -0.0163, -0.1134,  ...,  0.3029, -0.0267,  0.2478],\n",
       "         [-0.0087, -0.0172, -0.1129,  ...,  0.3021, -0.0270,  0.2475],\n",
       "         ...,\n",
       "         [-0.0105, -0.0171, -0.1128,  ...,  0.3028, -0.0264,  0.2474],\n",
       "         [-0.0108, -0.0169, -0.1136,  ...,  0.3027, -0.0270,  0.2481],\n",
       "         [-0.0103, -0.0157, -0.1135,  ...,  0.3021, -0.0265,  0.2476]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b75df",
   "metadata": {},
   "source": [
    "## MHA\n",
    "\n",
    "\n",
    "- the Wq Qk Wv are still the shape (hidden_size, hidden_size) on init, but will reshape to (hidden_size, num_heads, head_dim) when calling forward \n",
    "- Q K V shape will be (batch_size, seqlen, num_heads, head_dim)\n",
    "- Q K^T needs output of (batch_size, num_heads, seqlen, seqlen), so transpose seqlen and num_heads\n",
    "- scale use head_dim, not hidden_size\n",
    "- concat all heads after attention_score @ V (batch_size, num_heads, seqlen, head_dim) -> (batch_size, seqlen, num_heads * head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26f285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(SelfAttention):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__(hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = self.head_dim ** (1/2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seqlen, _ = x.shape\n",
    "        Q = self.Wq(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        K = self.Wk(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        V = self.Wv(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        \n",
    "        attention_score = softmax(Q.transpose(1,2) @ K.transpose(1,2).transpose(-1, -2) / self.scale, dim=-1)\n",
    "        attention_output = (attention_score @ V.transpose(1,2)).transpose(1,2).contiguous().view(bs, seqlen, -1)\n",
    "        return self.Wo(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07eedb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = MultiHeadSelfAttention(512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad7442a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2776,  0.0994,  0.1848,  ..., -0.0187,  0.1510,  0.0623],\n",
       "         [-0.2770,  0.0991,  0.1851,  ..., -0.0192,  0.1498,  0.0625],\n",
       "         [-0.2776,  0.0996,  0.1854,  ..., -0.0183,  0.1508,  0.0613],\n",
       "         ...,\n",
       "         [-0.2774,  0.0986,  0.1857,  ..., -0.0178,  0.1503,  0.0627],\n",
       "         [-0.2767,  0.0993,  0.1852,  ..., -0.0181,  0.1508,  0.0625],\n",
       "         [-0.2773,  0.0991,  0.1843,  ..., -0.0180,  0.1513,  0.0621]],\n",
       "\n",
       "        [[-0.2816,  0.1062,  0.1920,  ...,  0.0167,  0.1328,  0.0427],\n",
       "         [-0.2811,  0.1061,  0.1924,  ...,  0.0166,  0.1315,  0.0422],\n",
       "         [-0.2813,  0.1051,  0.1923,  ...,  0.0174,  0.1335,  0.0436],\n",
       "         ...,\n",
       "         [-0.2812,  0.1064,  0.1926,  ...,  0.0161,  0.1320,  0.0439],\n",
       "         [-0.2806,  0.1056,  0.1930,  ...,  0.0171,  0.1321,  0.0426],\n",
       "         [-0.2810,  0.1063,  0.1923,  ...,  0.0168,  0.1323,  0.0431]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354b276",
   "metadata": {},
   "source": [
    "## GQA\n",
    "\n",
    "- the Wq is the original size, Wk and Wv should be smaller\n",
    "- Wq is reshaped as (hidden_size, num_heads, head_dim), Wk and Wv is reshaped as (hidden_size, num_kv_heads, head_dim)\n",
    "- Q is (bs, seqlen, num_heads, head_dim), KV is  (bs, seqlen, num_kv_heads, head_dim), so need to interleave KV for group_size times, after repeating, KV is (bs, seqlen, num_heads, head_dim)\n",
    "- attention_score is still (bs, num_heads, seqlen, seqlen), after matmul V^T, is (bs, num_heads, seqlen, head_dim)\n",
    "\n",
    "note:\n",
    "`torch.repeat_interleave(input, repeats, dim=None, *, output_size=None)`\n",
    "\n",
    "- input (Tensor) – the input tensor.\n",
    "- repeats (Tensor or int) – The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.\n",
    "- dim (int, optional) – The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.\n",
    "\n",
    "```\n",
    ">>> torch.repeat_interleave(y, 3, dim=1)\n",
    "tensor([[1, 1, 1, 2, 2, 2],\n",
    "        [3, 3, 3, 4, 4, 4]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f3080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = self.head_dim ** (1/2)\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.group_size = num_heads // num_kv_heads\n",
    "        \n",
    "        self.Wq = Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Wk = Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wv = Linear(hidden_size, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.Wo = Linear(hidden_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seqlen, _ = x.shape\n",
    "        Q = self.Wq(x).view(bs, seqlen, self.num_heads, self.head_dim)\n",
    "        K = self.Wk(x).view(bs, seqlen, self.num_kv_heads, self.head_dim)\n",
    "        V = self.Wv(x).view(bs, seqlen, self.num_kv_heads, self.head_dim)\n",
    "        \n",
    "        K = K.repeat_interleave(self.group_size, dim=2)\n",
    "        V = V.repeat_interleave(self.group_size, dim=2)\n",
    "        \n",
    "        attention_score = (Q.transpose(1,2) @ K.transpose(1,2).transpose(-1,-2) / self.scale)\n",
    "        attention_output = (attention_score @ V.transpose(1,2)).transpose(1,2).contiguous().view(bs, seqlen, -1)\n",
    "        return self.Wo(attention_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246c9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = GroupedQueryAttention(512, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4cba9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2398, -0.2169,  0.0244,  ...,  0.5646, -0.1241,  0.1939],\n",
       "         [-0.0762, -0.0269,  0.1034,  ...,  0.3262, -0.0654,  0.0542],\n",
       "         [-0.0948,  0.1091,  0.4631,  ...,  0.3258, -0.3812,  0.3509],\n",
       "         ...,\n",
       "         [-0.0408, -0.3334,  0.0882,  ...,  0.8266, -0.1313,  0.3886],\n",
       "         [-0.0590,  0.1535,  0.2616,  ...,  0.0895, -0.4499,  0.1498],\n",
       "         [ 0.1171,  0.0685,  0.0594,  ...,  0.7256, -0.0245, -0.1301]],\n",
       "\n",
       "        [[-0.1654,  0.1116,  0.8133,  ...,  0.2510, -0.5537,  0.3507],\n",
       "         [ 0.3660, -0.1932,  0.4336,  ...,  0.7051,  0.1167,  0.2321],\n",
       "         [ 0.1229,  0.0439,  0.5826,  ...,  0.4699, -0.0516,  0.3244],\n",
       "         ...,\n",
       "         [ 0.3033,  0.1550,  0.3179,  ...,  0.5955, -0.0863,  0.2315],\n",
       "         [ 0.3834,  0.2093, -0.5854,  ..., -0.0066, -0.1727,  0.3564],\n",
       "         [ 0.3823, -0.1722,  0.2567,  ...,  0.3105, -0.3939,  0.4147]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.rand(2, 32, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaed9e1",
   "metadata": {},
   "source": [
    "## KV Cache\n",
    "\n",
    "- the forward just need to handle 1 token at a time, means seqlen==1, so the input X shape is (bs, 1, hidden_size)\n",
    "- past_kv = (past_key, past_value), both in shape of (bs, seqlen, hidden_size)\n",
    "- for new kv, concat past and this token's on dim=1, so now the Q is (bs, 1, hidden_size), new KV is (bs, seqlen+1, hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "650534c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWithKVCache(SelfAttention):\n",
    "    \n",
    "    def forward(self, x, cached_kv=None):\n",
    "        bs, _, hidden_size = x.shape\n",
    "        if cached_kv:\n",
    "            cached_key, cached_value = cached_kv\n",
    "        else:\n",
    "            cached_key = torch.zeros(bs, 0, hidden_size)\n",
    "            cached_value = torch.zeros(bs, 0, hidden_size)\n",
    "        \n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        cached_key = torch.cat([cached_key, K], dim=1)\n",
    "        cached_value = torch.cat([cached_value, V], dim=1)\n",
    "        \n",
    "        attention_score = softmax(Q @ cached_key.transpose(-1, -2) / self.scale, dim=-1)\n",
    "        output = self.Wo(attention_score @ cached_value)\n",
    "        \n",
    "        return output, (cached_key, cached_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b08801",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SelfAttentionWithKVCache(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418d084",
   "metadata": {},
   "source": [
    "### Time with KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68dbb1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2263050079345703\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "output, kv_cache = layer(torch.rand(2, 4096, 512))\n",
    "prefilling_time_cost = time.time() - start_time\n",
    "print(prefilling_time_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aad836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021359920501708984\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4097, 512]) torch.Size([2, 4097, 512])\n",
      "0.013353347778320312\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4098, 512]) torch.Size([2, 4098, 512])\n",
      "0.004659891128540039\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4099, 512]) torch.Size([2, 4099, 512])\n",
      "0.004498958587646484\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4100, 512]) torch.Size([2, 4100, 512])\n",
      "0.006295204162597656\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 4101, 512]) torch.Size([2, 4101, 512])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    start_time = time.time()\n",
    "    output, kv_cache = layer(torch.rand(2, 1, 512), kv_cache)\n",
    "    decoding_time_cost = time.time() - start_time\n",
    "    print(decoding_time_cost)\n",
    "    print(output.shape, kv_cache[0].shape, kv_cache[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b79b7",
   "metadata": {},
   "source": [
    "### Time without KV Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a32e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1712038516998291\n",
      "torch.Size([2, 4096, 512])\n",
      "0.15407514572143555\n",
      "torch.Size([2, 4097, 512])\n",
      "0.154710054397583\n",
      "torch.Size([2, 4098, 512])\n",
      "0.14727115631103516\n",
      "torch.Size([2, 4099, 512])\n",
      "0.14984130859375\n",
      "torch.Size([2, 4100, 512])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    start_time = time.time()\n",
    "    output, _ = layer(torch.rand(2, 4096 + _, 512), None)\n",
    "    decoding_time_cost = time.time() - start_time\n",
    "    print(decoding_time_cost)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd803504",
   "metadata": {},
   "source": [
    "## Heterogeneous Attention\n",
    "\n",
    "\n",
    "- the code demo is based on basic self attention, instead of the original implementation on GQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousKVCache:\n",
    "    def __init__(self, layer_idx):\n",
    "        self.layer_idx = layer_idx\n",
    "        self.sink = None\n",
    "        self.context = None\n",
    "        self.recency = None\n",
    "        \n",
    "    \n",
    "    def build(self):\n",
    "        pass\n",
    "    \n",
    "    def offload(self):\n",
    "        pass\n",
    "    \n",
    "    def search(self, q_states, top_k):\n",
    "        # ... mock some output here\n",
    "        selected_indices = list(range(top_k))\n",
    "        return selected_indices\n",
    "    \n",
    "    \n",
    "class HeterogeneousAttention(SelfAttentionWithKVCache): # or: SelfAttentionWithHeterogeneousKVCache\n",
    "    def __init__(self, hidden_size, layer_idx):\n",
    "        super().__init__(hidden_size)\n",
    "        self.heterogeneous_kv_cache = HeterogeneousKVCache(layer_idx)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
